{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "# 处理数据\r\n",
        "\r\n",
        "数据是构建机器学习模型的基础。在云中集中管理数据，并使在多个工作站上运行试验和训练模型的数据科学家团队能够访问这些数据以及计算目标，这是任何专业数据科学解决方案的重要组成部分。\r\n",
        "\r\n",
        "在该笔记本中，你将探索两个用于数据处理的 Azure 机器学习对象：数据存储和数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## 连接到工作区\r\n",
        "\r\n",
        "首先，请连接到你的工作区。\r\n",
        "\r\n",
        "> **备注**：如果尚未与 Azure 订阅建立经过身份验证的会话，则系统将提示你通过执行以下操作进行身份验证：单击链接，输入验证码，然后登录到 Azure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## 使用数据存储\r\n",
        "\r\n",
        "在 Azure ML 中，数据存储是对存储位置（例如 Azure 存储 Blob 容器）的引用。每个工作区都有一个默认的数据存储，该存储通常是使用相应工作区创建的 Azure 存储 Blob 容器。如果需要使用存储在不同位置的数据，则可以将自定义数据存储添加到工作区中，并将其中任何一个设置为默认值。\r\n",
        "\r\n",
        "### 查看数据存储\r\n",
        "\r\n",
        "运行以下代码以确定工作区中的数据存储："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "你还可以在 [Azure 机器学习工作室](https://ml.azure.com)中工作区的“**数据集**”页面上查看和管理工作区中的数据存储。\r\n",
        "\r\n",
        "### 将数据上传到数据存储\r\n",
        "\r\n",
        "确定可用的数据存储后，可以将文件从本地文件系统上传到数据存储，这样无论试验脚本实际在何处运行，工作区中运行的试验都能访问相应文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'], # Upload the diabetes csv files in /data\n",
    "                       target_path='diabetes-data/', # Put it in a folder path in the datastore\n",
    "                       overwrite=True, # Replace existing files of the same name\n",
    "                       show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## 使用数据集\r\n",
        "\r\n",
        "Azure 机器学习以数据集的形式提供数据的抽象。数据集是对可能要在试验中使用的一组特定数据的版本控制引用。数据集可以采用表格格式，也可以采用文件格式。\r\n",
        "\r\n",
        "### 创建表格数据集\r\n",
        "\r\n",
        "接下来根据上传到数据存储的糖尿病数据创建数据集，然后查看前 20 条记录。这种情况下，数据在 CSV 文件中采用结构化格式，因此我们将使用表格数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "\n",
    "# Display the first 20 rows as a Pandas dataframe\n",
    "tab_data_set.take(20).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "如上述代码中所见，可以轻松地将表格数据集转换为 Pandas 数据帧，从而使用常见的 python 技术处理数据。\r\n",
        "\r\n",
        "### 创建文件数据集\r\n",
        "\r\n",
        "你创建的数据集是表格数据集，可以在数据集定义所包含的结构化文件中作为包含所有数据的数据帧读取。这对于表格数据非常有效，但在某些机器学习场景中，可能需要使用非结构化数据；或者你可能只想通过自己的代码读取文件中的数据。为此，可以使用文件数据集，该数据集在虚拟装入点创建文件路径列表，用于读取文件中的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a file dataset from the path on the datastore (this may take a short while)\n",
    "file_data_set = Dataset.File.from_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "\n",
    "# Get the files in the dataset\n",
    "for file_path in file_data_set.to_path():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "### 注册数据集\r\n",
        "\r\n",
        "创建引用糖尿病数据的数据集后，可以将其注册，确保工作区中运行的所有试验可轻松对其进行访问。\r\n",
        "\r\n",
        "我们将表格数据集注册为“**糖尿病数据集**”，将文件数据集注册为“**糖尿病文件**”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                        name='diabetes dataset',\n",
    "                                        description='diabetes data',\n",
    "                                        tags = {'format':'CSV'},\n",
    "                                        create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "# Register the file dataset\n",
    "try:\n",
    "    file_data_set = file_data_set.register(workspace=ws,\n",
    "                                            name='diabetes file dataset',\n",
    "                                            description='diabetes files',\n",
    "                                            tags = {'format':'CSV'},\n",
    "                                            create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print('Datasets registered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "你可以在 [Azure 机器学习工作室](https://ml.azure.com)中工作区的“**数据集**”页面上查看和管理数据集。你还可以从工作区对象获取数据集列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Datasets:\")\n",
    "for dataset_name in list(ws.datasets.keys()):\n",
    "    dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "    print(\"\\t\", dataset.name, 'version', dataset.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "通过对数据集进行版本控制，可以重新定义数据集，从而无需破坏依赖先前定义的现有试验或管道。默认返回最新版本的已命名数据集，但可以通过指定版本号检索特定版本的数据集，如下所示：\r\n",
        "\r\n",
        "```python\r\n",
        "dataset_v1 = Dataset.get_by_name(ws, 'diabetes dataset', version = 1)\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "### 从表格数据集训练模型\r\n",
        "\r\n",
        "有数据集后，即可开始从中训练模型。可以在运行脚本的估算器中将数据集作为输入传递给脚本。\r\n",
        "\r\n",
        "运行以下两个代码单元格，创建以下内容：\r\n",
        "\r\n",
        "1. 名为 **diabetes_training_from_tab_dataset** 的文件夹\r\n",
        "2. 通过使用作为参数传递给分类模型的表格数据集来训练分类模型的脚本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = 'diabetes_training_from_tab_dataset'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "from azureml.core import Run, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Get the script arguments (regularization rate and training dataset ID)\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
    "parser.add_argument(\"--input-data\", type=str, dest='training_dataset_id', help='training dataset')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set regularization hyperparameter (passed as an argument to the script)\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get the training dataset\n",
    "print(\"Loading Data...\")\n",
    "diabetes = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "> **备注**：在脚本中，数据集作为形参（或实参）传递。对于表格数据集，此参数将包含已注册数据集的 ID，因此，你可以在脚本中编写代码以从运行上下文中获取试验的工作区，然后使用其 ID 获取数据集，如下所示：\r\n",
        ">\r\n",
        "> ```\r\n",
        "> run = Run.get_context()\r\n",
        "> ws = run.experiment.workspace\r\n",
        "> dataset = Dataset.get_by_id(ws, id=args.training_dataset_id)\r\n",
        "> diabetes = dataset.to_pandas_dataframe()\r\n",
        "> ```\r\n",
        ">\r\n",
        "> 但是，Azure 机器学习运行时会自动识别引用命名数据集的参数并将其添加到运行的 **input_datasets** 集合中，因此你还可以通过指定其“易记名称”来从该集合检索数据集（稍后你将看到，它在试验的脚本运行配置中的参数定义中指定）。这是上面脚本中采用的方法。\r\n",
        "\r\n",
        "现在，可以试验方式运行脚本，为训练数据集定义一个由脚本读取的参数。\r\n",
        "\r\n",
        "> **备注**：“数据集”类依赖于 **azureml-dataprep** 包中的一些组件，因此需要将此包包含在将运行训练试验的环境中。**azureml-dataprep** 包包含在 **azure-defaults** 包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "env = Environment.from_conda_specification(\"experiment_env\", \"environment.yml\")\n",
    "\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                              script='diabetes_training.py',\n",
    "                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n",
    "                              environment=env) \n",
    "\n",
    "# submit the experiment\n",
    "experiment_name = 'mslearn-train-diabetes'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "> **备注**： **--input-data** 参数将数据集作为指定输入传递，其中包括该数据集的易记名称，脚本在试验运行中使用该名称从 **input_datasets** 集合读取它。**--input-data** 参数中的字符串值实际上是已注册数据集的 ID。作为一种替代方法，可以只传递 `diabetes_ds.id`，在这种情况下，脚本可以从脚本参数访问数据集 ID，并使用该 ID 从工作区（而不是从 **input_datasets** 集合）获取数据集。\r\n",
        "\r\n",
        "首次运行试验时，可能需要一些时间来设置 Python 环境 - 后续运行会更快。\r\n",
        "\r\n",
        "试验完成后，在小组件中查看 **azureml-logs/70_driver_log.txt** 输出日志和运行所生成的指标。\r\n",
        "\r\n",
        "### 注册训练后的模型\r\n",
        "\r\n",
        "与任何训练试验一样，可以检索训练后的模型并在 Azure 机器学习工作区中注册它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
    "                   tags={'Training context':'Tabular dataset'}, properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "### 从文件数据集训练模型\r\n",
        "\r\n",
        "你已了解了如何使用表格数据集中的训练数据来训练模型。但文件数据集呢？\r\n",
        "\r\n",
        "使用文件数据集时，传递给脚本的数据集参数表示包含文件路径的装入点。从这些文件中读取数据的方式取决于文件中的数据类型及其预期用途。对于糖尿病 CSV 文件，可以使用 Python **glob** 模块在数据集定义的虚拟装入点中创建文件列表，并将其全部读入可联结为单个数据帧的 Pandas 数据帧中。\r\n",
        "\r\n",
        "运行以下两个代码单元格，创建以下内容：\r\n",
        "\r\n",
        "1. 名为 **diabetes_training_from_file_dataset** 的文件夹\r\n",
        "2. 使用文件数据集（作为输入传递给脚本）训练分类模型的脚本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = 'diabetes_training_from_file_dataset'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "from azureml.core import Dataset, Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import glob\n",
    "\n",
    "# Get script arguments (rgularization rate and file dataset mount point)\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
    "parser.add_argument('--input-data', type=str, dest='dataset_folder', help='data mount point')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set regularization hyperparameter (passed as an argument to the script)\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "data_path = run.input_datasets['training_files'] # Get the training data path from the input\n",
    "# (You could also just use args.dataset_folder if you don't want to rely on a hard-coded friendly name)\n",
    "\n",
    "# Read the files\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "diabetes = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "与表格数据集一样，你可以使用其易记名称从 **input_datasets** 集合中检索文件数据集。还可以从脚本参数检索它，对于文件数据集，脚本参数包含文件的安装路径（而不是传递给表格数据集的数据集 ID）。\r\n",
        "\r\n",
        "接下来需要更改将数据集传递到脚本的方式 - 这需要定义脚本可以从中读取文件的路径。可以使用 **as_download** 或 **as_mount** 方法来执行此操作。使用 **as_download** 会将文件数据集中的文件下载到计算机上运行脚本的临时位置，而 **as_mount** 会创建一个装入点，可以从该装入点直接从数据集传输文件。\r\n",
        "\r\n",
        "可以将访问方法与 **as_named_input** 方法结合使用，以在试验运行中将数据集包含在 **input_datasets** 集合中（如果不这样做，例如，通过将参数设置为 `diabetes_ds.as_mount()`，则脚本将能够从脚本参数（而不是从 **input_datasets** 集合）访问数据集装入点）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"diabetes file dataset\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='diabetes_training.py',\n",
    "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                             '--input-data', diabetes_ds.as_named_input('training_files').as_download()], # Reference to dataset location\n",
    "                                environment=env) # Use the environment created previously\n",
    "\n",
    "# submit the experiment\n",
    "experiment_name = 'mslearn-train-diabetes'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "试验完成后，在小组件中查看 **azureml-logs/70_driver_log.txt** 输出日志，以验证文件数据集中的文件已下载到临时文件夹中，从而使脚本能够读取文件。\r\n",
        "\r\n",
        "### 注册训练后的模型\r\n",
        "\r\n",
        "同样，可以注册由试验训练的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
    "                   tags={'Training context':'File dataset'}, properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "> **详细信息**：有关使用数据集进行训练的详细信息，请参阅 Azure ML 文档中的[使用数据集进行训练](https://docs.microsoft.com/azure/machine-learning/how-to-train-with-datasets)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}